<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="../css/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../css/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../css/print.css" media="print" />

    <title>Trevor's Robotics and Computer Vision Website.</title>
  </head>

  <body>
    <header>
      <div class="container">
        <h1>Trevor's Robotics Website</h1>
        <h2>A website to showcase my Robotics, Computer Vision and general technical projects.</h2>

        <section id="downloads">
          <a href="../docs/Trevor_Sherrard_Resume.pdf" class="btn btn-resume"><span class="icon"></span>Resume</a>
          <a href="https://github.com/sherrardTr4129" class="btn btn-github"><span class="icon"></span>GitHub</a>
          <a href="robotics_experience.html" class=btn btn-github>Robotics Projects</a>
          <a href="ComputerVision.html" class=btn btn-github>CV Projects</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h3>
            <a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Autonomous Camera Assistance for Dexterous Telemanipulation</h3>
           <p>This page outlines initial research work that I performed at Worchester Polytechnic Institute on the subject of autonomous camera assistance for dexterous telemanipulation task completion. The goal of this work was to design a viewpoint selection algorithm that would maximize image saliency in the frames shown to the teleoperator. This algorithm was implemented on the Trina2 system developed in the human inspired robotics laboratory at WPI. This robot feature two 7-DOF arms on a mobile robot base. These arms each have a gripper and eye-in-hand camera. The right arm was used  as the autonomous camera arm, and would seak out viewpoint that maximized image saliency for the teleoperator. While the right arm was used as the manually controlled arm for completeing the given telemanipulation task. The task designed to demonstrate the effectiveness of saliency maximization was a wall crack repair/inspection. In this task, the user would manually guide the manipulator arm over the cracks within the wall to "repair them", while the autonomous camera arm would seek to maximize saliency within images selected. The cracks are highly salient as compared to the rest of the wall, so choosing viewpoints that maximize the saliency in a given image should also maximize the cracks seen within a given image. The robot positioned within the tasking environment in the Gazebo simulation enviornment can be seen in the figure below.</p>

	<img src="../images/wallCrack.png" alt="Wall Crack Simulation" height= "800" width= "600">
	<br><br>
	<h3><a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Flask Web-Based Graphical User Interface</h3>
	
	<p>Once the robot was found to be setup correctly within the Gazebo environement, a custom GUI was implemented to allow for direct control of the end-effector's degrees of feedom. This GUI used a web front-end/back-end software architecture. In this way, the front end could be easily modified to run on a tablet, mobile phone or a standard computer, and the backend would work with the frontend in each of these cases. The frontend was designed using html/CSS and javascript. The backend was implemented using the Flask microservice framework, and standard ROS packages and python bindings. The backend outputs the current state of the frontend by publishing standard sensor_msgs/Joy messages over the /virtualJoystick topic. This allows the GUI to be easily extended into other works and applications. The software architecture diagram of the web-GUI along with a demonstration video of user input actuating the robot's end-effector Pose can be seen below.</p>

	<img src="../images/webGUIArch.png" alt="Web GUI Software Architecture" height= "650" width= "600">
	<br><br>
	<iframe width="600" height="315" src="https://www.youtube.com/embed/uFzn_BYGW-Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

	<p>The developed software used to run the frontend and backend components of the webGUI can be found in the ROS package <a href="https://github.com/sherrardTr4129/RBE526-Human-Robot-Interaction/tree/master/hri_web_gui">here</a></p>

	<h3><a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Saliency Maximization Algorithm</h3>
	<p>More to Come Soon!</p>
    </section>
    </div>
  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-65376015-1', 'auto');
  ga('send', 'pageview');

  </script>
</html>
