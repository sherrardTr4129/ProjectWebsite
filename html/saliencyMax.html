<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="../css/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../css/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../css/print.css" media="print" />

    <title>Trevor's Robotics and Computer Vision Website.</title>
  </head>

  <body>
    <header>
      <div class="container">
        <h1>Trevor's Robotics Website</h1>
        <h2>A website to showcase my Robotics, Computer Vision and general technical projects.</h2>

        <section id="downloads">
          <a href="../docs/Trevor_Sherrard_Resume.pdf" class="btn btn-resume"><span class="icon"></span>Resume</a>
          <a href="https://github.com/sherrardTr4129" class="btn btn-github"><span class="icon"></span>GitHub</a>
          <a href="robotics_experience.html" class=btn btn-github>Robotics Projects</a>
          <a href="ComputerVision.html" class=btn btn-github>CV Projects</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h3>
            <a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Autonomous Camera Assistance for Dexterous Telemanipulation</h3>
           <p>This page outlines initial research work that I performed at Worchester Polytechnic Institute on the subject of autonomous camera assistance for dexterous telemanipulation task completion. The goal of this work was to design a viewpoint selection algorithm that would maximize image saliency in the frames shown to the teleoperator. This algorithm was implemented on the Trina2 system developed in the human inspired robotics laboratory at WPI. This robot feature two 7-DOF arms on a mobile robot base. These arms each have a gripper and eye-in-hand camera. The right arm was used  as the autonomous camera arm, and would seak out viewpoint that maximized image saliency for the teleoperator. While the right arm was used as the manually controlled arm for completeing the given telemanipulation task. The task designed to demonstrate the effectiveness of saliency maximization was a wall crack repair/inspection. In this task, the user would manually guide the manipulator arm over the cracks within the wall to "repair them", while the autonomous camera arm would seek to maximize saliency within images selected. The cracks are highly salient as compared to the rest of the wall, so choosing viewpoints that maximize the saliency in a given image should also maximize the cracks seen within a given image. The robot positioned within the tasking environment in the Gazebo simulation enviornment can be seen in the figure below.</p>

	<img src="../images/wallCrack.png" alt="Wall Crack Simulation" height= "800" width= "600">
	<br><br>
	<h3><a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Flask Web-Based Graphical User Interface</h3>
	
	<p>Once the robot was found to be setup correctly within the Gazebo environement, a custom GUI was implemented to allow for direct control of the end-effector's degrees of feedom. This GUI used a web front-end/back-end software architecture. In this way, the front end could be easily modified to run on a tablet, mobile phone or a standard computer, and the backend would work with the frontend in each of these cases. The frontend was designed using html/CSS and javascript. The backend was implemented using the Flask microservice framework, and standard ROS packages and python bindings. The backend outputs the current state of the frontend by publishing standard sensor_msgs/Joy messages over the /virtualJoystick topic. This allows the GUI to be easily extended into other works and applications. The software architecture diagram of the web-GUI along with a demonstration video of user input actuating the robot's end-effector Pose can be seen below.</p>

	<img src="../images/webGUIArch.png" alt="Web GUI Software Architecture" height= "650" width= "600">
	<br><br>
	<iframe width="600" height="315" src="https://www.youtube.com/embed/uFzn_BYGW-Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

	<p>The developed software used to run the frontend and backend components of the webGUI can be found in the ROS package <a href="https://github.com/sherrardTr4129/RBE526-Human-Robot-Interaction/tree/master/hri_web_gui">here</a></p>

	<h3><a name="welcome-to-github-pages" class="anchor" href="Kudos.html#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Saliency Maximization Algorithm</h3>
	<p>The saliency maximization viewpoint selection algorithm was designed as a visual servoing algorithm of sorts. This means that the maximum (or local maximums) of image saliency within the enviornment were found by incrementally movimg in the direction of saliency maximization. If no direction would further maximize the saliency in the selected viewpoint, then saliency was said to be maximized. The developed saliency maximization algortihm can be seen in a flowchart form in the image below.</p>

	<img src="../images/saliencyMaximizaton.png" alt="Web GUI Software Architecture" height= "750" width= "600">

	<p>It can be seen that the algorithm can be split into two components, the saliency countour detection portion and distance minimization portion. The saliency coutour detection portion constructs a saliency map of the given frame, and finds highly salient regions that fit a set of filtering criteria. From here, the distance between detected contour center of mass and the center of the image is computed. This process is then repeated with image centers shifted in the four cardinal directions. The original computed distances are then compared to the shifted distances to see what proposed movement would best reduce the overall distance of saliency conours to the center of the image. The direction with the most distances reduced by a given threshold is chosen as the direction to move. The direction to move is used to generate a pose offset in the given direction to the autonomous camera arm's current pose. A demonstration of the saliency maximization algorithm running on a webcam using manual movements from the user in the direction suggested by the algorithm can be seen in the video below. Note that the green blobs are filterd saliency contours, the black lines represent the distance between the contours COM and the red arrow is the suggested direction to move.</p>

	<iframe width="600" height="315" src="https://www.youtube.com/embed/XT1T-kBxdh8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

	<p>This same algorithm was run using images taken from the autonomous camera arm of the Trina2 robot arm. Unfortunatley, the time required to generate and execute trajectories by the MoveIt! motion planners resulted in the image topics needing to be throttled down to sub-hertz frequencies. Due to this system limition, the responsiveness of the robot to saliency information is jerky and infrequent. This algorithm can be seen running on the Trina2 system in the video below.</p>

	<iframe width="600" height="315" src="https://www.youtube.com/embed/bX2uKEfQwgo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

	<p>The generated saliency maximization algorithm can be found in the ROS package <a href="https://github.com/sherrardTr4129/RBE526-Human-Robot-Interaction/tree/master/saliency_to_pose">here</a></p>

      </section>
    </div>
  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-65376015-1', 'auto');
  ga('send', 'pageview');

  </script>
</html>
